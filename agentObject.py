import os
import dspy
from datetime import datetime, timezone
from dotenv import load_dotenv
from dbObject import dbObject

class Agent():
    def __init__(self, network_id: str, agent_id=None, db: dbObject=None):
        if db is None:
            raise ValueError("Database object cannot be None.")
        self.db = db
        self.network_id = network_id

        if network_id is None:
            raise ValueError("Network ID cannot be None.")

        self.agent_id = agent_id
        
        self._setup_language_model()
        self.user_chat_module = UserChatModule()
        self.instructions = self.db.get_instructions(self.agent_id)
        self.chat_history = []

    def _setup_language_model(self):
        self.turbo = dspy.OpenAI(model="gpt-3.5-turbo", max_tokens=2000, model_type="chat", temperature=0.8) 
        dspy.settings.configure(lm=self.turbo)
        print("Language model configured successfully.")

######################## USER CHAT ########################

########### need to completely rework this for 
    # just responds to a message
    def handle_user_chat(self, message):
        user_response = self.user_chat_module(prompt=message, instructions=self.instructions)
        self.chat_history.append({"user": message, "agent": user_response["answer"]})
    
        return user_response["answer"]


######################### AGENTS CHAT ########################
        
    def handle_agent_interaction(self, partner_agent_id):
        # init DSPy chat modules
        home_agent = AgentChatModule(self.db, self.agent_id, self.chat_history)
        away_agent = AgentChatModule(self.db, partner_agent_id, self.chat_history)

        # initial prompt to start the conversation
        # TODO: logic could be added for the initial prompt to be generated by what's in the memory retrevial, or maybe what's most recent
        init_prompt = "Hey, talk to me!"
        
        # chat interactions duration
        for i in range(5): # TODO: logic should be added here to determine how many interactions they have
            if i == 0:
                # home responds to init prompt
                home_retrieved_memories = str(self.db.get_agent_memory(self.agent_id, init_prompt))
                home_response = home_agent.forward(prompt=init_prompt, instructions=self.instructions, retrieved_memories=home_retrieved_memories)

                # away responds to home's response
                away_retrieved_memories = str(self.db.get_agent_memory(partner_agent_id, home_response['answer']))
                away_response = away_agent.forward(prompt=home_response['answer'], instructions=self.instructions, retrieved_memories=away_retrieved_memories)

            else:
                # home responds to init prompt
                home_retrieved_memories = str(self.db.get_agent_memory(self.agent_id, init_prompt))
                # print(home_retrieved_memories)
                home_response = home_agent.forward(prompt=away_response['answer'], instructions=self.instructions, retrieved_memories=home_retrieved_memories)

                # away responds to home's response
                away_retrieved_memories = str(self.db.get_agent_memory(partner_agent_id, home_response['answer']))
                # print(away_retrieved_memories)
                away_response = away_agent.forward(prompt=home_response['answer'], instructions=self.instructions, retrieved_memories=away_retrieved_memories)

            # maybe we add a DSPy module for deciding if the conversation should continue or not 
            # we should use llms for these types of decisions to demonstrate the extent of the capabilities
        # summarize chat history (to store in memory)
            # memory retrevial in chats needs to be designed to interpret the format of the summary correctly. (think about how it will be used in the prompt)
            # then we can do things like store the memories with "On Monday, May 31, 2024, I talked to Agent 2 about..."
            # that logic still needs to figured out
                
        summarizer = ChatHistorySummarizer()
        interaction_summary = summarizer(self.agent_id, self.chat_history)

        # check toxicity of the chat history
        formatted_chat_history = home_agent.format_chat_history() # make it a str
        toxicity_checker = ToxicityChecker(self.db)
        is_toxic = toxicity_checker(formatted_chat_history, self.agent_id)

        # store summary in memory
        self.db.add_agent_data(self.agent_id, interaction_summary, is_toxic)

        # testing   
        print(f"Chat history: \n{formatted_chat_history}")
        print(f"Chat history summary: \n{interaction_summary}")

        return interaction_summary


########################## DSPY CLASSES ##########################

##################### AGENT CHAT MODULES #####################
class AgentChatModule(dspy.Module):
    class AgentChatSignature(dspy.Signature):
        """
        Exchange information with another agent, following the instructions provided. Do not make up any information or experiences.
        Find commonalities and relevant things in your memory retrieval based on what the other agent asks you.
        """
        guidelines = dspy.InputField(desc="User-set guidelines for you follow during social interactions.")
        prompt = dspy.InputField(desc="A message from the other agent.")
        memory_retrieval = dspy.OutputField(desc="Retrieved memory based on the prompt.")
        answer = dspy.OutputField(desc="A response to the other agent.")

    def __init__(self, db, agent_id, chat_history):
        self.db = db
        self.agent_id = agent_id
        self.chat_history = chat_history # stored in the class so we can call it outside of the module

    def forward(self, prompt, instructions, retrieved_memories):
        # add some module here for interpeting the memories based on how we store their summaries. Picking the ones relevant to the conversation to compose context info string to be added to the prompt.

        relevance_fixer = RelevanceFixer()

        response = relevance_fixer(
            instructions=instructions,
            prompt=prompt,
            retrieved_memories=retrieved_memories,
            max_retries=3
        )

        # Add the latest response in the chat history
        self.append_chat_history(prompt, response["answer"])

        return response

    def append_chat_history(self, prompt, response):
        timestamp = datetime.now(timezone.utc).isoformat()
        chat_entry = {
            "timestamp": timestamp,
            "agent_id": self.agent_id,
            "prompt": prompt,
            "response": response
        }
        self.chat_history.append(chat_entry)

    # for better interpretation during summary
    def format_chat_history(self):
        formatted_history = []
        for entry in self.chat_history:
            formatted_entry = f"Timestamp: {entry['timestamp']}\n"
            formatted_entry += f"Agent {entry['agent_id']}\nPrompt: {entry['prompt']}\n"
            formatted_entry += f"Response: {entry['response']}\n"
            formatted_entry += "-" * 50
            formatted_history.append(formatted_entry + "\n")
        return "\n".join(formatted_history)
    

##################### EVALUATIONS/CHECKERS #####################
class RelevanceFixer(dspy.Module):
    class RelevanceFixerSignature(dspy.Signature):
        """
        Generate a relevant response based on the prompt and retrieved memories. If the initial response is not relevant, retry up to max_retries times.
        """
        guidelines = dspy.InputField(desc="User-set guidelines for the agent to follow during social interactions.")
        prompt = dspy.InputField()
        retrieved_memories = dspy.InputField()
        max_retries = dspy.InputField(desc="Maximum number of retries.")
        answer = dspy.OutputField(desc="A response to the prompt.")

    # i wonder if we should be passing a model object into the init so that we remain consistent with the correct agent logic happening with the correct agent's model

    # makes sure that a response is relevant to the prompt and the retrieved memories, this is like our halucination fixer
    # it uses a checker module in this flow to decide if it needs to retry
    def forward(self, prompt, instructions, retrieved_memories, max_retries):
        relevance_checker = self.RelevanceChecker()
        # print("\n\n\n\n\n")
        # print(retrieved_memories)
        # print("\n\n\n\n\n")
        for attempt in range(max_retries):
            if attempt > 0:
                prompt = f"The prompt is: '{prompt}'. The previous attempted response was not relevant to the context, let's try again."
            
            initial_response = dspy.ChainOfThought(
                AgentChatModule.AgentChatSignature, 
                rationale_type=dspy.OutputField(
                    prefix="Reasoning: Let's think step by step in order to",
                    desc="respond casually, either drawing connections between the prompt and retrieved memory, or bringing up things from your memory if the prompt is not relevant. We ...",
                )
                )(
                    guidelines=instructions,
                    prompt=prompt,
                    memory_retrieval=retrieved_memories
                ).answer

            # relevancy checker (returns boolean)
            is_relevant = relevance_checker(
                prompt=prompt,
                response=initial_response,
                retrieved_memories=retrieved_memories
            )

            # retry if not relevant
            if is_relevant:
                return {"answer": initial_response}
            else:
                if attempt < max_retries - 1:
                    print(f"Attempt {attempt + 1} failed. Retrying...")
                else:
                    # TODO: we should change this to a default that won't derail the convo
                    return {"answer": "Sorry, I couldn't generate a relevant response based on my memories."} 
                
    # checker module
    class RelevanceChecker(dspy.Module):
        class ValidateRelevanceSignature(dspy.Signature):
            """
            Decide if the response makes sense, it should:
            - be relevant to the prompt
            - be relevant to the retrieved memories
            Answer with 'Yes' if the response is relevant to the memories, otherwise answer with 'No'.
            """
            
            prompt = dspy.InputField(desc="Prompt to which we are responding")
            response = dspy.InputField(desc="Potential response to the prompt")
            retrieved_memories = dspy.InputField(desc="Retrieved memories")
            answer = dspy.OutputField(desc="Yes or No")

        def forward(self, prompt, response, retrieved_memories):
            rationale_type = dspy.OutputField(
                prefix="Reasoning: Let's think step by step in order to",
                desc="Figure out if this response aligns with the conversation and our goal of making connections based on things in our memory, without making anything up. ...",
            )

            result = dspy.ChainOfThought(self.ValidateRelevanceSignature, rationale_type=rationale_type)(
                prompt=prompt,
                response=response,
                retrieved_memories=retrieved_memories
            ).answer

            return "Yes" in result

class ToxicityChecker(dspy.Module):
    class ToxicityCheckerSignature(dspy.Signature):
        """
        Check the toxicity of chat history and return 'Yes' if contains anything deemed as toxic by the settings, otherwise return 'No'.
        Chat history should not include anything that is off limits according to the toxicity settings.
        """
        toxicity_settings = dspy.InputField(desc="User settings for what is considered toxic.")
        chat_history = dspy.InputField(desc="Chat history to check for toxicity")
        in_context_prompt = dspy.InputField(desc="In-context prompt for the agent")
        answer = dspy.OutputField(desc="Yes or No")

    def __init__(self, db: dbObject):
        self.db = db

    def forward(self, chat_history, home_agent_id):
        # call toxicity settings from db
        toxicity_settings = self.db.get_toxicty_settings(home_agent_id)

        rationale_type = dspy.OutputField(
            prefix="Reasoning: Let's think step by step in order to",
            desc="Check if the chat history contains anything that the user settings consider toxic.",
        )

        result = dspy.ChainOfThought(self.ToxicityCheckerSignature, rationale_type=rationale_type)(
            toxicity_settings=toxicity_settings,
            chat_history=chat_history,
        ).answer

        print(result)
        return 'Yes' in result

    # I'm thinking we should create another tag like the toxicity flag, but for positive things. Like if it finds anything highly relevant gets a good tag. 

##################### USER MODULES #####################

# handles casual chat with the user
class UserChatModule(dspy.Module):
    class UserChatSignature(dspy.Signature):
        """Your task is to be a casual texting buddy of the user, texting with abbreviations and common slang. You can ask questions, provide answers, or just chat. You must follow the settings/instructions given to you."""
        guidelines = dspy.InputField(desc="User-set guidelines for the agent to follow during social interactions.")
        prompt = dspy.InputField()
        answer = dspy.OutputField(desc="A response to the user.")
        update_command = dspy.OutputField(desc="Command to update settings, if detected. Optional")
    
    def forward(self, prompt, instructions):
        # default casual chat response
        response = dspy.ChainOfThought(self.UserChatSignature)(guidelines=instructions, prompt=prompt).answer
        return {"answer": response}
    
        # for now, we'll streamline setting instructions, and toxicity_settings.
            # they can be set via their endpoints/dbObject methods
            # later, we can make them also updateable via the chat, by adding modules/checks here. For now, keep it simple stupid (KISS)
            # self.db.set_instructions(self.agent_id, instructions)
            # self.db.set_toxicity_settings(self.agent_id, toxicity_settings)
    

##################### CHAT HISTORY SUMMARIZER #####################
class ChatHistorySummarizer(dspy.Module):
    class ChatHistorySummarySignature(dspy.Signature):
        """
        Summarize the chat history with very descriptive information about what was shared. You are writing a one paragraph briefing for someone when someone asks you about the same topic.
        """
        chat_history = dspy.InputField(desc="The chat history to be summarized.")
        summary = dspy.OutputField(desc="A descriptive summary of the chat history.")
        # I really think we should include some examples of the format we want them to be stored in. They should include things like the date, away_agent_id, etc. This will be decided by how we interpret it on the recall side.
        # the module that turns the memories into conversation specific context should be able to interpret the format of the summary correctly.
        
    def forward(self, home_agent_id, chat_history_list):
        away_agent_id = chat_history_list[0]['agent_id'] if chat_history_list[0]['agent_id'] != home_agent_id else chat_history_list[1]['agent_id']
        rationale_type = dspy.OutputField(
            prefix="Reasoning: Let's think step by step in order to",
            desc=f"accurately summarize the chat history with very descriptive information about what was shared. I talked to {away_agent_id} about...",
        )

        chat_history_str = ""
        for item in chat_history_list:
            label = 'Me' if item['agent_id'] == home_agent_id else f'Agent {item["agent_id"]}'
            prompt_label = f'{label}: {item["prompt"]}' if label != 'Me' else label
            chat_history_str += f"{prompt_label}\n{label}: {item['response']}\n\n"


        response = dspy.ChainOfThought(self.ChatHistorySummarySignature, rationale_type=rationale_type)(
            chat_history=chat_history_str
        ).summary

        return response